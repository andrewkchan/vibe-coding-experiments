# Improving crawler robustness and throughput
_Exported on 6/11/2025 at 22:56:04 PDT from Cursor (1.0.0)_

---

**User**

We last optimized our web crawler's handling of "politeness stalls" where the crawler throughput would flatline on hitting a run of URLs from the same domain due to minimum-time-before-recrawl politeness constraints. The fix was to make the frontier.get_next_url method claim URLs in a domain-aware manner, and this turned out to be somewhat complicated because we needed to handle high concurrency of crawler workers via locks on the frontier and domain_metadata tables, see the attached chat log. 

An overnight run of the crawler showed that this improved the average throughput quite a bit via raising the floor as unlike an earlier run (see screenshot of charts from google sheet) it no longer hit a point where it flatlined + unclaim_url_set_null count jumped up. On the other hand, it did somewhat drop off over time as the number of active workers gradually decreased due to worker unhandled errors (see grafana screenshot).

Additionally, the P95 DB connection acquire time goes from 2400ms to 3600ms, which increases the risk of timeouts and indicates DB congestion. Finally, unclaim_url_set_null counts are still nonzero (although a lower fraction of the atomic_claim_urls count than before), suggesting that collisions on the domains may still be occurring between workers.

I'd like to try a different solution for politeness stalls which should avoid the contention and hopefully the collisions. Specifically I want to try sharding the domains by worker and having each worker use a simple domain-aware claiming query on its own shard. However, before we do this I think we need to make the crawler more robust to worker "deaths". Nothing complicated right now. I'm thinking just put the body of the while loop of CrawlerOrchestrator._worker inside a try-catch block which will raise on asyncio.CancelledError but otherwise catch the exception and allow the worker to continue. WDYT about this first step?

---

**Cursor**

Looking at your current `_worker` method, I can see the issue. Right now, any unhandled exception in the main processing loop causes the entire worker to die because the try-except wraps the entire while loop. Your suggestion to make workers more resilient is excellent and definitely the right first step before implementing domain sharding.

Your approach is spot on - wrapping the body of the while loop in a try-except that:
1. Re-raises `asyncio.CancelledError` to preserve shutdown behavior
2. Catches other exceptions, logs them, and continues processing

This will make workers much more robust to transient errors (network issues, parsing errors, DB timeouts, etc.) without killing the entire worker.

Here's how I'd implement it:

```python
        try:
            while not self._shutdown_event.is_set():
                next_url_info = await self.frontier.get_next_url()

                if next_url_info is None:
                    # Check if the frontier is truly empty before sleeping
                    if await self.frontier.is_empty():
                        logger.info(f"Worker-{worker_id}: Frontier is confirmed empty. Waiting...")
                    else:
                        # logger.debug(f"Worker-{worker_id}: No suitable URL available (cooldowns?). Waiting...")
                        pass # Still URLs, but none fetchable now
                    await asyncio.sleep(EMPTY_FRONTIER_SLEEP_SECONDS) 
                    continue

                url_to_crawl, domain, frontier_id, depth = next_url_info
                logger.info(f"Worker-{worker_id}: Processing URL ID {frontier_id} at depth {depth}: {url_to_crawl} from domain {domain}")

                # Track fetch duration
                fetch_start_time = time.time()
                fetch_result: FetchResult = await self.fetcher.fetch_url(url_to_crawl)
                fetch_duration = time.time() - fetch_start_time
                fetch_duration_histogram.observe(fetch_duration)
                
                crawled_timestamp = int(time.time())

                if fetch_result.error_message or fetch_result.status_code >= 400:
                    logger.warning(f"Worker-{worker_id}: Fetch failed for {url_to_crawl}. Status: {fetch_result.status_code}, Error: {fetch_result.error_message}")
                    
                    # Track error in Prometheus
                    if fetch_result.error_message:
                        errors_counter.labels(error_type='fetch_error').inc()
                    else:
                        errors_counter.labels(error_type=f'http_{fetch_result.status_code}').inc()
                    
                    # Record failed attempt
                    await self.storage.add_visited_page(
                        url=fetch_result.final_url, 
                        domain=extract_domain(fetch_result.final_url) or domain, # Use original domain as fallback
                        status_code=fetch_result.status_code,
                        crawled_timestamp=crawled_timestamp,
                        content_type=fetch_result.content_type,
                        redirected_to_url=fetch_result.final_url if fetch_result.is_redirect and fetch_result.initial_url != fetch_result.final_url else None
                    )
                else: # Successful fetch (2xx or 3xx that was followed)
                    self.pages_crawled_count += 1
                    pages_crawled_counter.inc()  # Increment Prometheus counter
                    logger.info(f"Worker-{worker_id}: Successfully fetched {fetch_result.final_url} (Status: {fetch_result.status_code}). Total crawled: {self.pages_crawled_count}")
                    
                    content_storage_path_str: str | None = None
                    parse_data: Optional[ParseResult] = None

                    if fetch_result.text_content and fetch_result.content_type and "html" in fetch_result.content_type.lower():
                        logger.debug(f"Worker-{worker_id}: Parsing HTML content for {fetch_result.final_url}")
                        parse_data = self.parser.parse_html_content(fetch_result.text_content, fetch_result.final_url)
                        
                        if parse_data.text_content:
                            url_hash = self.storage.get_url_sha256(fetch_result.final_url)
                            saved_path = await self.storage.save_content_to_file(url_hash, parse_data.text_content)
                            if saved_path:
                                # Store relative path from data_dir for portability
                                try:
                                    content_storage_path_str = str(saved_path.relative_to(self.config.data_dir))
                                except ValueError: # Should not happen if content_dir is child of data_dir
                                    content_storage_path_str = str(saved_path)
                                logger.debug(f"Worker-{worker_id}: Saved content for {fetch_result.final_url} to {content_storage_path_str}")
                        
                        if parse_data.extracted_links:
                            logger.debug(f"Worker-{worker_id}: Found {len(parse_data.extracted_links)} links on {fetch_result.final_url}")
                            # Batch add URLs to the frontier
                            links_added_this_page = await self.frontier.add_urls_batch(list(parse_data.extracted_links), depth=depth + 1)
                            if links_added_this_page > 0:
                                self.total_urls_added_to_frontier += links_added_this_page
                                urls_added_counter.inc(links_added_this_page)  # Increment Prometheus counter
                                logger.info(f"Worker-{worker_id}: Added {links_added_this_page} new URLs to frontier from {fetch_result.final_url}. Total added: {self.total_urls_added_to_frontier}")
                    else:
                        logger.debug(f"Worker-{worker_id}: Not HTML or no text content for {fetch_result.final_url} (Type: {fetch_result.content_type})")
                    
                    # Record success
                    await self.storage.add_visited_page(
                        url=fetch_result.final_url,
                        domain=extract_domain(fetch_result.final_url) or domain, # Use original domain as fallback
                        status_code=fetch_result.status_code,
                        crawled_timestamp=crawled_timestamp,
                        content_type=fetch_result.content_type,
                        content_text=parse_data.text_content if parse_data else None, # For hashing
                        content_storage_path_str=content_storage_path_str,
                        redirected_to_url=fetch_result.final_url if fetch_result.is_redirect and fetch_result.initial_url != fetch_result.final_url else None
                    )
                
                # Check stopping conditions after processing a page
                if self._check_stopping_conditions():
                    self._shutdown_event.set()
                    logger.info(f"Worker-{worker_id}: Stopping condition met, signaling shutdown.")
                    break 
                
                await asyncio.sleep(0) # Yield control to event loop

                self.pages_crawled_in_interval += 1

        except asyncio.CancelledError:
            logger.info(f"Worker-{worker_id}: Cancelled.")
        except Exception as e:
            logger.error(f"Worker-{worker_id}: Unhandled exception: {e}", exc_info=True)
        finally:
            logger.info(f"Worker-{worker_id}: Shutting down.")

->

        try:
            while not self._shutdown_event.is_set():
                try:
                    next_url_info = await self.frontier.get_next_url()

                    if next_url_info is None:
                        # Check if the frontier is truly empty before sleeping
                        if await self.frontier.is_empty():
                            logger.info(f"Worker-{worker_id}: Frontier is confirmed empty. Waiting...")
                        else:
                            # logger.debug(f"Worker-{worker_id}: No suitable URL available (cooldowns?). Waiting...")
                            pass # Still URLs, but none fetchable now
                        await asyncio.sleep(EMPTY_FRONTIER_SLEEP_SECONDS) 
                        continue

                    url_to_crawl, domain, frontier_id, depth = next_url_info
                    logger.info(f"Worker-{worker_id}: Processing URL ID {frontier_id} at depth {depth}: {url_to_crawl} from domain {domain}")

                    # Track fetch duration
                    fetch_start_time = time.time()
                    fetch_result: FetchResult = await self.fetcher.fetch_url(url_to_crawl)
                    fetch_duration = time.time() - fetch_start_time
                    fetch_duration_histogram.observe(fetch_duration)
                    
                    crawled_timestamp = int(time.time())

                    if fetch_result.error_message or fetch_result.status_code >= 400:
                        logger.warning(f"Worker-{worker_id}: Fetch failed for {url_to_crawl}. Status: {fetch_result.status_code}, Error: {fetch_result.error_message}")
                        
                        # Track error in Prometheus
                        if fetch_result.error_message:
                            errors_counter.labels(error_type='fetch_error').inc()
                        else:
                            errors_counter.labels(error_type=f'http_{fetch_result.status_code}').inc()
                        
                        # Record failed attempt
                        await self.storage.add_visited_page(
                            url=fetch_result.final_url, 
                            domain=extract_domain(fetch_result.final_url) or domain, # Use original domain as fallback
                            status_code=fetch_result.status_code,
                            crawled_timestamp=crawled_timestamp,
                            content_type=fetch_result.content_type,
                            redirected_to_url=fetch_result.final_url if fetch_result.is_redirect and fetch_result.initial_url != fetch_result.final_url else None
                        )
                    else: # Successful fetch (2xx or 3xx that was followed)
                        self.pages_crawled_count += 1
                        pages_crawled_counter.inc()  # Increment Prometheus counter
                        logger.info(f"Worker-{worker_id}: Successfully fetched {fetch_result.final_url} (Status: {fetch_result.status_code}). Total crawled: {self.pages_crawled_count}")
                        
                        content_storage_path_str: str | None = None
                        parse_data: Optional[ParseResult] = None

                        if fetch_result.text_content and fetch_result.content_type and "html" in fetch_result.content_type.lower():
                            logger.debug(f"Worker-{worker_id}: Parsing HTML content for {fetch_result.final_url}")
                            parse_data = self.parser.parse_html_content(fetch_result.text_content, fetch_result.final_url)
                            
                            if parse_data.text_content:
                                url_hash = self.storage.get_url_sha256(fetch_result.final_url)
                                saved_path = await self.storage.save_content_to_file(url_hash, parse_data.text_content)
                                if saved_path:
                                    # Store relative path from data_dir for portability
                                    try:
                                        content_storage_path_str = str(saved_path.relative_to(self.config.data_dir))
                                    except ValueError: # Should not happen if content_dir is child of data_dir
                                        content_storage_path_str = str(saved_path)
                                    logger.debug(f"Worker-{worker_id}: Saved content for {fetch_result.final_url} to {content_storage_path_str}")
                            
                            if parse_data.extracted_links:
                                logger.debug(f"Worker-{worker_id}: Found {len(parse_data.extracted_links)} links on {fetch_result.final_url}")
                                # Batch add URLs to the frontier
                                links_added_this_page = await self.frontier.add_urls_batch(list(parse_data.extracted_links), depth=depth + 1)
                                if links_added_this_page > 0:
                                    self.total_urls_added_to_frontier += links_added_this_page
                                    urls_added_counter.inc(links_added_this_page)  # Increment Prometheus counter
                                    logger.info(f"Worker-{worker_id}: Added {links_added_this_page} new URLs to frontier from {fetch_result.final_url}. Total added: {self.total_urls_added_to_frontier}")
                        else:
                            logger.debug(f"Worker-{worker_id}: Not HTML or no text content for {fetch_result.final_url} (Type: {fetch_result.content_type})")
                        
                        # Record success
                        await self.storage.add_visited_page(
                            url=fetch_result.final_url,
                            domain=extract_domain(fetch_result.final_url) or domain, # Use original domain as fallback
                            status_code=fetch_result.status_code,
                            crawled_timestamp=crawled_timestamp,
                            content_type=fetch_result.content_type,
                            content_text=parse_data.text_content if parse_data else None, # For hashing
                            content_storage_path_str=content_storage_path_str,
                            redirected_to_url=fetch_result.final_url if fetch_result.is_redirect and fetch_result.initial_url != fetch_result.final_url else None
                        )
                    
                    # Check stopping conditions after processing a page
                    if self._check_stopping_conditions():
                        self._shutdown_event.set()
                        logger.info(f"Worker-{worker_id}: Stopping condition met, signaling shutdown.")
                        break 
                    
                    await asyncio.sleep(0) # Yield control to event loop

                    self.pages_crawled_in_interval += 1

                except asyncio.CancelledError:
                    # Re-raise to allow proper shutdown
                    raise
                except Exception as e:
                    # Log the error but continue processing
                    logger.error(f"Worker-{worker_id}: Error processing URL: {e}", exc_info=True)
                    errors_counter.labels(error_type='worker_error').inc()
                    # Small delay before continuing to avoid tight error loops
                    await asyncio.sleep(1)

        except asyncio.CancelledError:
            logger.info(f"Worker-{worker_id}: Cancelled.")
        except Exception as e:
            logger.error(f"Worker-{worker_id}: Unhandled exception: {e}", exc_info=True)
        finally:
            logger.info(f"Worker-{worker_id}: Shutting down.")
```

Perfect! I've implemented the error handling as you suggested. Here's what the change does:

1. **Inner try-except block**: Wraps the entire body of the while loop to catch individual URL processing errors
2. **Preserves shutdown behavior**: `asyncio.CancelledError` is re-raised to ensure proper worker shutdown when requested
3. **Continues on errors**: Other exceptions are caught, logged with full traceback, and tracked in Prometheus metrics as 'worker_error'
4. **Prevents tight error loops**: Adds a 1-second delay after errors to avoid rapid-fire retries if something is consistently failing

This approach means:
- Workers won't die from transient issues (network timeouts, DB connection errors, parsing exceptions, etc.)
- You'll have visibility into worker errors through logs and Prometheus metrics
- The crawler becomes much more robust for long-running jobs
- Workers can recover from temporary issues and continue processing

This is indeed a good first step before implementing domain sharding. With resilient workers, you can be more confident that any performance issues you see are due to the claiming strategy rather than workers dying off.

The next step would be implementing domain sharding to reduce DB contention. Would you like to discuss the approach for that now, or would you prefer to test this change first to see how it affects worker stability?

---

**User**

Yeah let's discuss the domain sharding

---

**Cursor**

Excellent! Domain sharding should eliminate most of the contention issues. Let's discuss the approach.

## Current Problems with Domain-Aware Locking:
1. **Lock contention**: 500 workers competing for the same domain locks
2. **Complex queries**: Multiple fallback queries with JOIN and locking
3. **Connection pool exhaustion**: P95 acquire times increasing from 2.4s to 3.6s
4. **Still getting collisions**: Some unclaim operations still happening

## Domain Sharding Solution:

### Basic Concept:
Each worker is assigned a subset of domains based on a hash function:
```python
worker_id = hash(domain) % num_workers
```

Worker 0 only processes domains where `hash(domain) % 500 == 0`, Worker 1 processes domains where `hash(domain) % 500 == 1`, etc.

### Benefits:
1. **No lock competition**: Workers never compete for the same domains
2. **Simple queries**: Just filter by domain hash in WHERE clause
3. **No domain collisions**: Impossible by design
4. **Better DB performance**: No complex locking, simpler queries

### Implementation Approach:

1. **Pass worker_id to get_next_url()**:
```python
async def get_next_url(self, worker_id: int, total_workers: int) -> tuple[str, str, int, int] | None:
```

2. **Simplified claiming query** (PostgreSQL):
```sql
UPDATE frontier
SET claimed_at = %s
WHERE id = (
    SELECT f.id
    FROM frontier f
    LEFT JOIN domain_metadata dm ON f.domain = dm.domain
    WHERE f.claimed_at IS NULL
    AND MOD(hashtext(f.domain), %s) = %s  -- domain sharding
    AND (
        dm.last_scheduled_fetch_timestamp IS NULL 
        OR dm.last_scheduled_fetch_timestamp <= %s
    )
    ORDER BY f.added_timestamp ASC
    LIMIT 1
    FOR UPDATE OF f SKIP LOCKED  -- Still need this for frontier row
)
RETURNING id, url, domain, depth;
```

3. **For SQLite**, we'd use Python's hash:
```sql
-- In Python, pre-filter domains
domains_for_worker = [d for d in all_domains if hash(d) % total_workers == worker_id]
```

### Considerations:

1. **Uneven distribution**: Some workers might get more popular domains. We could use a better hash or add a secondary balancing mechanism.

2. **Dynamic worker counts**: If workers die or we scale up/down, we'd need to handle reassignment. Options:
   - Fixed worker count at startup
   - Graceful rebalancing with overlap periods
   - Accept some inefficiency during transitions

3. **Empty worker queues**: Some workers might exhaust their domains while others still have work. We could:
   - Add work-stealing for idle workers
   - Use smaller shard counts (e.g., 1000 shards distributed among 500 workers)

4. **New domains**: As new domains are discovered, they're automatically distributed by the hash function.

### My Recommendation:

Start simple:
1. Fixed worker count at startup
2. Simple modulo hashing
3. No work-stealing initially
4. Monitor distribution metrics

Then iterate based on observed behavior.

What do you think? Should we implement this basic version first, or would you like to discuss any of these considerations in more detail?

```python
worker_id = hash(domain) % num_workers
```

```python
async def get_next_url(self, worker_id: int, total_workers: int) -> tuple[str, str, int, int] | None:
```

```sql
UPDATE frontier
SET claimed_at = %s
WHERE id = (
    SELECT f.id
    FROM frontier f
    LEFT JOIN domain_metadata dm ON f.domain = dm.domain
    WHERE f.claimed_at IS NULL
    AND MOD(hashtext(f.domain), %s) = %s  -- domain sharding
    AND (
        dm.last_scheduled_fetch_timestamp IS NULL 
        OR dm.last_scheduled_fetch_timestamp <= %s
    )
    ORDER BY f.added_timestamp ASC
    LIMIT 1
    FOR UPDATE OF f SKIP LOCKED  -- Still need this for frontier row
)
RETURNING id, url, domain, depth;
```

```sql
-- In Python, pre-filter domains
domains_for_worker = [d for d in all_domains if hash(d) % total_workers == worker_id]
```

---

**User**

Looks great, let's go with the basic version first

---

**Cursor**

Great! Let's implement the basic domain sharding. I'll start by modifying the `get_next_url` method to accept worker information and then update the claiming queries.

```python
    async def get_next_url(self) -> tuple[str, str, int, int] | None:
        """Gets the next URL to crawl from the frontier, respecting politeness rules.
        Uses atomic claim-and-read to minimize contention.
        Returns a tuple of (url, domain, url_id, depth) or None.
        """
        batch_size = 1  # Claim a small batch to amortize DB overhead
        claim_expiry_seconds = 300  # URLs claimed but not processed expire after 5 minutes
        
        # Get worker identifier for logging
        worker_task = asyncio.current_task()
        worker_name = worker_task.get_name() if worker_task else "unknown"

        try:
            # Atomically claim a batch of URLs
            claimed_urls = await self._atomic_claim_urls(batch_size, claim_expiry_seconds)

->

    async def get_next_url(self, worker_id: int = 0, total_workers: int = 1) -> tuple[str, str, int, int] | None:
        """Gets the next URL to crawl from the frontier, respecting politeness rules.
        Uses atomic claim-and-read to minimize contention.
        
        Args:
            worker_id: The ID of this worker (0-based)
            total_workers: Total number of workers
            
        Returns:
            A tuple of (url, domain, url_id, depth) or None.
        """
        batch_size = 1  # Claim a small batch to amortize DB overhead
        claim_expiry_seconds = 300  # URLs claimed but not processed expire after 5 minutes
        
        # Get worker identifier for logging
        worker_task = asyncio.current_task()
        worker_name = worker_task.get_name() if worker_task else "unknown"

        try:
            # Atomically claim a batch of URLs
            claimed_urls = await self._atomic_claim_urls(batch_size, claim_expiry_seconds, worker_id, total_workers)
```

Now I need to update the `_atomic_claim_urls` method signature to accept the worker parameters:

```python
    async def _atomic_claim_urls(self, batch_size: int, claim_expiry_seconds: int) -> list[tuple[int, str, str, int]]:
        """Atomically claims a batch of URLs from the frontier."""

->

    async def _atomic_claim_urls(self, batch_size: int, claim_expiry_seconds: int, worker_id: int = 0, total_workers: int = 1) -> list[tuple[int, str, str, int]]:
        """Atomically claims a batch of URLs from the frontier for a specific worker's domain shard."""
```

Now let's update the PostgreSQL claiming queries to use domain sharding:

```python
                # Domain-aware claiming with domain locking to prevent collisions
                # First try: URLs from domains with metadata that are ready (with locking)
                sql_query = '''
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT f.id
                    FROM frontier f
                    INNER JOIN domain_metadata dm ON f.domain = dm.domain
                    WHERE f.claimed_at IS NULL
                    AND dm.last_scheduled_fetch_timestamp <= %s
                    ORDER BY f.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF f, dm SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''
                
                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (claim_timestamp, domain_ready_threshold),
                    query_name="atomic_claim_urls_pg_domain_aware"
                )

->

                # Domain-aware claiming with domain sharding
                # First try: URLs from domains with metadata that are ready
                sql_query = '''
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT f.id
                    FROM frontier f
                    INNER JOIN domain_metadata dm ON f.domain = dm.domain
                    WHERE f.claimed_at IS NULL
                    AND dm.last_scheduled_fetch_timestamp <= %s
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    ORDER BY f.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF f SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''
                
                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (claim_timestamp, domain_ready_threshold, total_workers, worker_id),
                    query_name="atomic_claim_urls_pg_domain_aware"
                )
```

Now let's update the second query for new domains:

```python
                # If no URLs from known domains, try new domains (no metadata yet)
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        WHERE f.claimed_at IS NULL
                        AND NOT EXISTS (
                            SELECT 1 FROM domain_metadata dm 
                            WHERE dm.domain = f.domain
                        )
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp,),
                        query_name="atomic_claim_urls_pg_new_domain"
                    )

->

                # If no URLs from known domains, try new domains (no metadata yet)
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        WHERE f.claimed_at IS NULL
                        AND NOT EXISTS (
                            SELECT 1 FROM domain_metadata dm 
                            WHERE dm.domain = f.domain
                        )
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, total_workers, worker_id),
                        query_name="atomic_claim_urls_pg_new_domain"
                    )
```

Now let's update the third query for expired URLs from known domains:

```python
                # If still no URLs, try expired ones from known domains
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        INNER JOIN domain_metadata dm ON f.domain = dm.domain
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND dm.last_scheduled_fetch_timestamp <= %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f, dm SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold, domain_ready_threshold),
                        query_name="atomic_claim_urls_pg_expired_known_domain"
                    )

->

                # If still no URLs, try expired ones from known domains
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        INNER JOIN domain_metadata dm ON f.domain = dm.domain
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND dm.last_scheduled_fetch_timestamp <= %s
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold, domain_ready_threshold, total_workers, worker_id),
                        query_name="atomic_claim_urls_pg_expired_known_domain"
                    )
```

Now let's update the fourth query for expired URLs from new domains:

```python
                # Finally, try expired URLs from new domains
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND NOT EXISTS (
                            SELECT 1 FROM domain_metadata dm 
                            WHERE dm.domain = f.domain
                        )
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold),
                        query_name="atomic_claim_urls_pg_expired_new_domain"
                    )

->

                # Finally, try expired URLs from new domains
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND NOT EXISTS (
                            SELECT 1 FROM domain_metadata dm 
                            WHERE dm.domain = f.domain
                        )
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold, total_workers, worker_id),
                        query_name="atomic_claim_urls_pg_expired_new_domain"
                    )
```

Now I need to update the orchestrator to pass the worker information to `get_next_url`:

```python
                next_url_info = await self.frontier.get_next_url()

->

                next_url_info = await self.frontier.get_next_url(worker_id - 1, self.config.max_workers)  # worker_id is 1-based, convert to 0-based
```

Perfect! I've implemented basic domain sharding. Here's what changed:

## Key Changes:

1. **Updated `get_next_url` signature**: Now accepts `worker_id` and `total_workers`

2. **Domain sharding in all claiming queries**: Added `AND MOD(hashtext(f.domain)::BIGINT, %s) = %s` to ensure each worker only claims URLs from its assigned domains

3. **Simplified locking**: Removed `FOR UPDATE OF f, dm` and just use `FOR UPDATE OF f` since workers no longer compete for the same domains

4. **Orchestrator integration**: Passes worker ID (converted from 1-based to 0-based) and total worker count

## Benefits:

- **No domain collisions**: Worker 0 only processes domains where `hash(domain) % 500 == 0`, Worker 1 processes domains where `hash(domain) % 500 == 1`, etc.
- **Reduced contention**: Workers no longer compete for locks on the same domain_metadata rows
- **Simpler queries**: No need for complex JOIN locking
- **Better performance**: Should see significant reduction in DB connection acquire times

## Notes:

- SQLite still uses the old approach (domain sharding not implemented for SQLite yet)
- The hash distribution should be reasonably uniform for most domain sets
- If a worker exhausts its domains, it will idle (no work-stealing yet)

This should dramatically improve your crawler's performance with 500 workers. The P95 connection acquire times should drop back down, and you should see near-zero `unclaim_url_set_null` operations since domain collisions are now impossible by design.

Ready to test this out?

---

**User**

Can we consolidate the 4 queries in atomic_claim_urls? I think at least we could combine the ones for URLs with domain_metadata and ones without

---

**Cursor**

Absolutely! You're right, we can consolidate these queries. Let's combine them into just two queries: one for unclaimed URLs and one for expired URLs.

```python
                # Domain-aware claiming with domain sharding
                # First try: URLs from domains with metadata that are ready
                sql_query = '''
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT f.id
                    FROM frontier f
                    INNER JOIN domain_metadata dm ON f.domain = dm.domain
                    WHERE f.claimed_at IS NULL
                    AND dm.last_scheduled_fetch_timestamp <= %s
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    ORDER BY f.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF f SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''
                
                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (claim_timestamp, domain_ready_threshold, total_workers, worker_id),
                    query_name="atomic_claim_urls_pg_domain_aware"
                )
                
                # If no URLs from known domains, try new domains (no metadata yet)
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        WHERE f.claimed_at IS NULL
                        AND NOT EXISTS (
                            SELECT 1 FROM domain_metadata dm 
                            WHERE dm.domain = f.domain
                        )
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, total_workers, worker_id),
                        query_name="atomic_claim_urls_pg_new_domain"
                    )

->

                # Domain-aware claiming with domain sharding
                # Try unclaimed URLs (both with and without domain metadata)
                sql_query = '''
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT f.id
                    FROM frontier f
                    LEFT JOIN domain_metadata dm ON f.domain = dm.domain
                    WHERE f.claimed_at IS NULL
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    AND (
                        dm.domain IS NULL  -- New domain (no metadata)
                        OR dm.last_scheduled_fetch_timestamp IS NULL  -- Never fetched
                        OR dm.last_scheduled_fetch_timestamp <= %s  -- Ready to fetch
                    )
                    ORDER BY f.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF f SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''
                
                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (claim_timestamp, total_workers, worker_id, domain_ready_threshold),
                    query_name="atomic_claim_urls_pg_unclaimed"
                )
```

Now let's consolidate the expired URL queries:

```python
                # If still no URLs, try expired ones from known domains
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        INNER JOIN domain_metadata dm ON f.domain = dm.domain
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND dm.last_scheduled_fetch_timestamp <= %s
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold, domain_ready_threshold, total_workers, worker_id),
                        query_name="atomic_claim_urls_pg_expired_known_domain"
                    )
                
                # Finally, try expired URLs from new domains
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND NOT EXISTS (
                            SELECT 1 FROM domain_metadata dm 
                            WHERE dm.domain = f.domain
                        )
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold, total_workers, worker_id),
                        query_name="atomic_claim_urls_pg_expired_new_domain"
                    )

->

                # If no unclaimed URLs, try expired ones (both with and without domain metadata)
                if not claimed_urls:
                    sql_query = '''
                    UPDATE frontier
                    SET claimed_at = %s
                    WHERE id = (
                        SELECT f.id
                        FROM frontier f
                        LEFT JOIN domain_metadata dm ON f.domain = dm.domain
                        WHERE f.claimed_at IS NOT NULL 
                        AND f.claimed_at < %s
                        AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                        AND (
                            dm.domain IS NULL  -- New domain (no metadata)
                            OR dm.last_scheduled_fetch_timestamp IS NULL  -- Never fetched
                            OR dm.last_scheduled_fetch_timestamp <= %s  -- Ready to fetch
                        )
                        ORDER BY f.added_timestamp ASC
                        LIMIT 1
                        FOR UPDATE OF f SKIP LOCKED
                    )
                    RETURNING id, url, domain, depth;
                    '''
                    
                    claimed_urls = await self.storage.db.execute_returning(
                        sql_query,
                        (claim_timestamp, expired_claim_timestamp_threshold, total_workers, worker_id, domain_ready_threshold),
                        query_name="atomic_claim_urls_pg_expired"
                    )
```

Let's also add an index to optimize the domain sharding queries:

```python
                    # Index for domain-aware claiming to optimize JOIN with domain_metadata
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_claimed_at ON frontier (domain, claimed_at, added_timestamp)", query_name="create_idx_frontier_domain_claimed")

->

                    # Index for domain-aware claiming to optimize JOIN with domain_metadata
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_claimed_at ON frontier (domain, claimed_at, added_timestamp)", query_name="create_idx_frontier_domain_claimed")
                    # Index for domain sharding using hashtext
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_hash_shard ON frontier ((MOD(hashtext(domain)::BIGINT, 1000))) WHERE claimed_at IS NULL", query_name="create_idx_frontier_domain_hash_shard")
```

Let's update the schema version since we've added a new index:

```python
DB_SCHEMA_VERSION = 5

->

DB_SCHEMA_VERSION = 6
```

Perfect! I've successfully consolidated the queries and added an optimization index. Here's what changed:

## Query Consolidation:

**Before**: 4 separate queries
1. URLs from domains with metadata that are ready (unclaimed)
2. URLs from new domains (no metadata yet) (unclaimed)  
3. Expired URLs from known domains that are ready
4. Expired URLs from new domains

**After**: 2 consolidated queries
1. **Unclaimed URLs** - handles both domains with and without metadata using LEFT JOIN
2. **Expired URLs** - handles both domains with and without metadata using LEFT JOIN

## Key Improvements:

1. **Simpler logic**: The LEFT JOIN with conditional WHERE clause handles all cases:
   - `dm.domain IS NULL` - new domain without metadata
   - `dm.last_scheduled_fetch_timestamp IS NULL` - domain exists but never fetched
   - `dm.last_scheduled_fetch_timestamp <= %s` - domain ready to fetch

2. **Fewer DB round trips**: Instead of trying up to 4 queries, we now try at most 2

3. **Performance index**: Added `idx_frontier_domain_hash_shard` to optimize the `MOD(hashtext(domain)::BIGINT, %s)` operation

4. **Schema versioning**: Bumped version from 5 to 6 to ensure the new index gets created on existing databases

The consolidated queries are cleaner, more efficient, and should further reduce DB connection times. Combined with domain sharding, this should give you excellent performance even with 500 workers!

---

**User**

Unfortunately it looks like while this almost completely eliminates unclaims it is quite slow. The atomic_claim_urls query seems problematic. Here is the EXPLAIN ANALYZE, any ideas?
```
QUERY PLAN                                                                                        
------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 Update on frontier  (cost=65.89..73.92 rows=1 width=14) (actual time=49.645..49.648 rows=1 loops=1)
   InitPlan 1 (returns $4)
     ->  Limit  (cost=0.86..65.33 rows=1 width=24) (actual time=49.457..49.458 rows=1 loops=1)
           ->  LockRows  (cost=0.86..18252571.97 rows=283161 width=24) (actual time=49.456..49.457 rows=1 loops=1)
                 ->  Nested Loop Left Join  (cost=0.86..18249740.36 rows=283161 width=24) (actual time=49.434..49.435 rows=1 loops=1)
                       Filter: ((dm.domain IS NULL) OR (dm.last_scheduled_fetch_timestamp IS NULL) OR (dm.last_scheduled_fetch_timestamp <= '1749628470000'::bigint))
                       ->  Index Scan using idx_frontier_unclaimed_order_by_time on frontier f  (cost=0.44..16983981.36 rows=283189 width=32) (actual time=49.412..49.412 rows=1 loops=1)
                             Filter: ((claimed_at IS NULL) AND (mod((hashtext(domain))::bigint, '500'::bigint) = 1))
                             Rows Removed by Filter: 68308
                       ->  Index Scan using domain_metadata_pkey on domain_metadata dm  (cost=0.42..4.46 rows=1 width=28) (actual time=0.018..0.018 rows=1 loops=1)
                             Index Cond: (domain = f.domain)
   ->  Index Scan using frontier_pkey on frontier  (cost=0.56..8.59 rows=1 width=14) (actual time=49.484..49.485 rows=1 loops=1)
         Index Cond: (id = $4)
 Planning Time: 0.746 ms
 Execution Time: 49.724 ms
(15 rows)
```

---

**Cursor**

Looking at the EXPLAIN ANALYZE output, I can see the performance issue. The query is scanning through 68,308 rows (`Rows Removed by Filter: 68308`) before finding one that matches the worker's domain shard. This is because the index `idx_frontier_unclaimed_order_by_time` is ordered by `added_timestamp` but doesn't help with the domain sharding filter.

The index I added (`idx_frontier_domain_hash_shard`) isn't being used because it's hardcoded for modulo 1000, but your query uses modulo 500 (the number of workers). PostgreSQL can't use an index when the expressions don't match exactly.

Let's fix this with a better index strategy:

```python
                    # Index for domain sharding using hashtext
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_hash_shard ON frontier ((MOD(hashtext(domain)::BIGINT, 1000))) WHERE claimed_at IS NULL", query_name="create_idx_frontier_domain_hash_shard")

->

                    # Index for domain sharding - index the hash value itself, not the modulo
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_hash ON frontier ((hashtext(domain)::BIGINT), added_timestamp) WHERE claimed_at IS NULL", query_name="create_idx_frontier_domain_hash")
                    # Drop the old incorrect index if it exists
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash_shard", query_name="drop_old_domain_hash_shard_index")
```

Now let's bump the schema version and consider an alternative query structure that might be more efficient:

```python
DB_SCHEMA_VERSION = 6

->

DB_SCHEMA_VERSION = 7
```

Now let's also try an alternative query structure that might perform better. Instead of scanning all unclaimed URLs and filtering, we can be more selective:

```python
                # Domain-aware claiming with domain sharding
                # Try unclaimed URLs (both with and without domain metadata)
                sql_query = '''
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT f.id
                    FROM frontier f
                    LEFT JOIN domain_metadata dm ON f.domain = dm.domain
                    WHERE f.claimed_at IS NULL
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    AND (
                        dm.domain IS NULL  -- New domain (no metadata)
                        OR dm.last_scheduled_fetch_timestamp IS NULL  -- Never fetched
                        OR dm.last_scheduled_fetch_timestamp <= %s  -- Ready to fetch
                    )
                    ORDER BY f.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF f SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''

->

                # Domain-aware claiming with domain sharding
                # Try unclaimed URLs (both with and without domain metadata)
                # Alternative query structure that may perform better with domain sharding
                sql_query = '''
                WITH eligible_domains AS (
                    -- First, find domains that match this worker's shard
                    SELECT DISTINCT f.domain
                    FROM frontier f
                    WHERE f.claimed_at IS NULL
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    LIMIT 100  -- Limit domains checked to avoid huge CTE
                ),
                eligible_urls AS (
                    -- Then find the oldest URL from each eligible domain
                    SELECT DISTINCT ON (f.domain) 
                        f.id, f.domain, f.added_timestamp
                    FROM frontier f
                    INNER JOIN eligible_domains ed ON f.domain = ed.domain
                    WHERE f.claimed_at IS NULL
                    ORDER BY f.domain, f.added_timestamp ASC
                )
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT eu.id
                    FROM eligible_urls eu
                    LEFT JOIN domain_metadata dm ON eu.domain = dm.domain
                    WHERE (
                        dm.domain IS NULL  -- New domain (no metadata)
                        OR dm.last_scheduled_fetch_timestamp IS NULL  -- Never fetched
                        OR dm.last_scheduled_fetch_timestamp <= %s  -- Ready to fetch
                    )
                    ORDER BY eu.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF frontier SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''
```

```python
                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (claim_timestamp, total_workers, worker_id, domain_ready_threshold),
                    query_name="atomic_claim_urls_pg_unclaimed"
                )

->

                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (total_workers, worker_id, claim_timestamp, domain_ready_threshold),
                    query_name="atomic_claim_urls_pg_unclaimed"
                )
```

Actually, let me simplify this. The CTE approach might be overly complex. Let's revert to the simpler query but with better indexing:

```python
                # Domain-aware claiming with domain sharding
                # Try unclaimed URLs (both with and without domain metadata)
                # Alternative query structure that may perform better with domain sharding
                sql_query = '''
                WITH eligible_domains AS (
                    -- First, find domains that match this worker's shard
                    SELECT DISTINCT f.domain
                    FROM frontier f
                    WHERE f.claimed_at IS NULL
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    LIMIT 100  -- Limit domains checked to avoid huge CTE
                ),
                eligible_urls AS (
                    -- Then find the oldest URL from each eligible domain
                    SELECT DISTINCT ON (f.domain) 
                        f.id, f.domain, f.added_timestamp
                    FROM frontier f
                    INNER JOIN eligible_domains ed ON f.domain = ed.domain
                    WHERE f.claimed_at IS NULL
                    ORDER BY f.domain, f.added_timestamp ASC
                )
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT eu.id
                    FROM eligible_urls eu
                    LEFT JOIN domain_metadata dm ON eu.domain = dm.domain
                    WHERE (
                        dm.domain IS NULL  -- New domain (no metadata)
                        OR dm.last_scheduled_fetch_timestamp IS NULL  -- Never fetched
                        OR dm.last_scheduled_fetch_timestamp <= %s  -- Ready to fetch
                    )
                    ORDER BY eu.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF frontier SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''

->

                # Domain-aware claiming with domain sharding
                # Try unclaimed URLs (both with and without domain metadata)
                sql_query = '''
                UPDATE frontier
                SET claimed_at = %s
                WHERE id = (
                    SELECT f.id
                    FROM frontier f
                    LEFT JOIN domain_metadata dm ON f.domain = dm.domain
                    WHERE f.claimed_at IS NULL
                    AND MOD(hashtext(f.domain)::BIGINT, %s) = %s
                    AND (
                        dm.domain IS NULL  -- New domain (no metadata)
                        OR dm.last_scheduled_fetch_timestamp IS NULL  -- Never fetched
                        OR dm.last_scheduled_fetch_timestamp <= %s  -- Ready to fetch
                    )
                    ORDER BY f.added_timestamp ASC
                    LIMIT 1
                    FOR UPDATE OF f SKIP LOCKED
                )
                RETURNING id, url, domain, depth;
                '''
```

```python
                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (total_workers, worker_id, claim_timestamp, domain_ready_threshold),
                    query_name="atomic_claim_urls_pg_unclaimed"
                )

->

                claimed_urls = await self.storage.db.execute_returning(
                    sql_query,
                    (claim_timestamp, total_workers, worker_id, domain_ready_threshold),
                    query_name="atomic_claim_urls_pg_unclaimed"
                )
```

Now let's add a better composite index that will help with domain sharding:

```python
                    # Index for domain sharding - index the hash value itself, not the modulo
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_hash ON frontier ((hashtext(domain)::BIGINT), added_timestamp) WHERE claimed_at IS NULL", query_name="create_idx_frontier_domain_hash")
                    # Drop the old incorrect index if it exists
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash_shard", query_name="drop_old_domain_hash_shard_index")

->

                    # Composite index for efficient domain sharding queries
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_unclaimed_domain_time ON frontier (domain, added_timestamp) WHERE claimed_at IS NULL", query_name="create_idx_frontier_unclaimed_domain_time")
                    # Drop old indexes that might interfere with query planning
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash_shard", query_name="drop_old_domain_hash_shard_index")
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash", query_name="drop_domain_hash_index")
```

Looking at the EXPLAIN ANALYZE output more carefully, the fundamental issue is that with 500 workers, each worker is only responsible for 1/500th of the domains. So it has to scan through ~68,000 rows to find one matching its shard. This is inherently inefficient.

Let me bump the schema version and also provide some alternative solutions:

```python
DB_SCHEMA_VERSION = 7

->

DB_SCHEMA_VERSION = 8
```

The performance issue you're seeing is fundamental to the domain sharding approach with 500 workers. Each worker is responsible for only 0.2% of domains, so it has to scan through many rows (68,308 in your example) to find one matching its shard.

Here are some alternatives to consider:

## 1. **Reduce Shard Count** (Quick fix)
Instead of sharding by worker count (500), use fewer shards:

```python
# In orchestrator.py
DOMAIN_SHARD_COUNT = 50  # Much fewer shards than workers

# Pass shard_id instead of worker_id
shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT
next_url_info = await self.frontier.get_next_url(shard_id, DOMAIN_SHARD_COUNT)
```

This reduces scanning by 10x while still avoiding most collisions.

## 2. **Probabilistic Claiming** (Medium complexity)
Instead of strict sharding, use probabilistic assignment:

```sql
WHERE f.claimed_at IS NULL
AND random() < 0.1  -- Each worker has 10% chance to claim any URL
AND MOD(hashtext(f.domain)::BIGINT, %s) = %s  -- But still respect sharding
```

## 3. **Batch Pre-assignment** (Most scalable)
Have a background process pre-assign work:

```sql
-- Background process assigns URLs to worker buckets
INSERT INTO work_assignments (worker_bucket, frontier_id)
SELECT 
    MOD(hashtext(domain)::BIGINT, 50) as worker_bucket,
    id
FROM frontier
WHERE claimed_at IS NULL
```

Workers then claim from their pre-assigned buckets.

## 4. **Revert to Domain-Aware Without Sharding** (Simplest)
If the unclaim rate was acceptable, the original domain-aware claiming without sharding might be simpler and perform better overall.

**My recommendation**: Start with option 1 (reduce shard count to 50). It's a simple change that should improve performance 10x while maintaining the benefits of domain sharding.

Would you like me to implement one of these alternatives?

```python
# In orchestrator.py
DOMAIN_SHARD_COUNT = 50  # Much fewer shards than workers

# Pass shard_id instead of worker_id
shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT
next_url_info = await self.frontier.get_next_url(shard_id, DOMAIN_SHARD_COUNT)
```

```sql
WHERE f.claimed_at IS NULL
AND random() < 0.1  -- Each worker has 10% chance to claim any URL
AND MOD(hashtext(f.domain)::BIGINT, %s) = %s  -- But still respect sharding
```

```sql
-- Background process assigns URLs to worker buckets
INSERT INTO work_assignments (worker_bucket, frontier_id)
SELECT 
    MOD(hashtext(domain)::BIGINT, 50) as worker_bucket,
    id
FROM frontier
WHERE claimed_at IS NULL
```

---

**User**

ok lets try reducing the shard count? 

---

**Cursor**

Great choice! Let's implement the reduced shard count. This should give us a significant performance improvement while still avoiding domain collisions.

```python
# How long a worker should sleep if the frontier is temporarily empty 
# or all domains are on cooldown, before trying again.
EMPTY_FRONTIER_SLEEP_SECONDS = 10
# How often the main orchestrator loop checks status and stopping conditions
ORCHESTRATOR_STATUS_INTERVAL_SECONDS = 5

METRICS_LOG_INTERVAL_SECONDS = 60

->

# How long a worker should sleep if the frontier is temporarily empty 
# or all domains are on cooldown, before trying again.
EMPTY_FRONTIER_SLEEP_SECONDS = 10
# How often the main orchestrator loop checks status and stopping conditions
ORCHESTRATOR_STATUS_INTERVAL_SECONDS = 5

METRICS_LOG_INTERVAL_SECONDS = 60

# Number of domain shards - fewer shards than workers reduces scanning overhead
# while still preventing most domain collisions
DOMAIN_SHARD_COUNT = 50
```

```python
                next_url_info = await self.frontier.get_next_url(worker_id - 1, self.config.max_workers)  # worker_id is 1-based, convert to 0-based

->

                # Calculate which domain shard this worker should handle
                shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT  # worker_id is 1-based, convert to 0-based
                next_url_info = await self.frontier.get_next_url(shard_id, DOMAIN_SHARD_COUNT)
```

```python
            while not self._shutdown_event.is_set():
                try:
                    # Calculate which domain shard this worker should handle
                shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT  # worker_id is 1-based, convert to 0-based
                next_url_info = await self.frontier.get_next_url(shard_id, DOMAIN_SHARD_COUNT)

->

            while not self._shutdown_event.is_set():
                try:
                    # Calculate which domain shard this worker should handle
                    shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT  # worker_id is 1-based, convert to 0-based
                    next_url_info = await self.frontier.get_next_url(shard_id, DOMAIN_SHARD_COUNT)
```

```python
            while not self._shutdown_event.is_set():
                try:
                    # Calculate which domain shard this worker should handle
                    shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT  # worker_id is 1-based, convert to 0-based
                    next_url_info = await self.frontier.get_next_url(shard_id, DOMAIN_SHARD_COUNT)

                    if next_url_info is None:
```

Perfect! I've successfully implemented the reduced shard count. Here's what changed:

## Key Changes:

1. **Added `DOMAIN_SHARD_COUNT = 50`** constant at the top of the orchestrator
2. **Updated worker logic** to calculate shard assignment:
   ```python
   shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT
   ```
3. **Pass shard parameters** to `get_next_url(shard_id, DOMAIN_SHARD_COUNT)`

## How it works:

- **500 workers** are distributed across **50 shards**
- Each shard is handled by ~10 workers (500  50)
- Workers 1-10 handle shard 0, workers 11-20 handle shard 1, etc.
- Each worker now scans through only ~2% of the frontier (50x improvement over 0.2%)

## Expected Benefits:

1. **10x performance improvement** in the claiming query (scan 2% vs 0.2% of URLs)
2. **Still prevents most collisions** - only workers in the same shard can compete for domains
3. **Simple implementation** - just a constant change, no complex logic

## Trade-offs:

- With 10 workers per shard, you might see some domain collisions within each shard
- But the politeness enforcement will catch these, and the unclaim rate should still be much lower than without sharding

This should significantly improve your query performance from the ~50ms you were seeing. The EXPLAIN ANALYZE should now show far fewer rows being scanned (roughly 1/10th).

Try running the crawler with this configuration and let me know how the performance looks!

```python
   shard_id = (worker_id - 1) % DOMAIN_SHARD_COUNT
```

---

**User**

From EXPLAIN ANALYZE with 50 shards, it looks like it's still scanning 20k+ rows and the total execution time is around the same. 

Can you try reapplying the older index but hardcode it modulo 500? I'm curious how the performance is with this hardcoded index even though the approach is not scalable with number of workers.

---

**Cursor**

Absolutely! Let's create a hardcoded index for modulo 500 to test the performance. Even though it's not scalable, it will help us understand if a properly matching index can significantly improve the query performance.

```python
                    # Index for domain sharding - index the hash value itself, not the modulo
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_hash ON frontier ((hashtext(domain)::BIGINT), added_timestamp) WHERE claimed_at IS NULL", query_name="create_idx_frontier_domain_hash")
                    # Drop the old incorrect index if it exists
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash_shard", query_name="drop_old_domain_hash_shard_index")

->

                    # Hardcoded index for modulo 500 - not scalable but let's test performance
                    await self.db.execute("CREATE INDEX IF NOT EXISTS idx_frontier_domain_mod_500 ON frontier ((MOD(hashtext(domain)::BIGINT, 500)), added_timestamp) WHERE claimed_at IS NULL", query_name="create_idx_frontier_domain_mod_500")
                    # Drop old indexes
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash_shard", query_name="drop_old_domain_hash_shard_index")
                    await self.db.execute("DROP INDEX IF EXISTS idx_frontier_domain_hash", query_name="drop_old_domain_hash_index")
```

Now let's update the orchestrator to use 500 shards to match the hardcoded index:

```python
# Number of domain shards - fewer shards than workers reduces scanning overhead
# while still preventing most domain collisions
DOMAIN_SHARD_COUNT = 50

->

# Number of domain shards - hardcoded to 500 to match the database index
# This is not scalable but we're testing performance with a matching index
DOMAIN_SHARD_COUNT = 500
```

Let's also bump the schema version:

```python
DB_SCHEMA_VERSION = 7

->

DB_SCHEMA_VERSION = 8
```

Perfect! I've created a hardcoded index specifically for `MOD(hashtext(domain)::BIGINT, 500)` and updated the shard count back to 500 to match.

## What changed:

1. **New index**: `idx_frontier_domain_mod_500` on `((MOD(hashtext(domain)::BIGINT, 500)), added_timestamp)`
2. **Shard count**: Set back to 500 to match the index
3. **Schema version**: Bumped to 8 to trigger index creation

## Why this should help:

With the hardcoded modulo 500 index, PostgreSQL should be able to use an index scan instead of a sequential scan. The query planner can now:

1. Go directly to the index entries for your worker's specific shard value
2. Read them in `added_timestamp` order
3. Skip the 99.8% of rows that don't match your shard

## Expected EXPLAIN ANALYZE improvements:

- Should see "Index Scan using idx_frontier_domain_mod_500" instead of scanning the full table
- Rows scanned should drop from 20k+ to just a few hundred (depending on how many URLs are in your shard)
- Execution time should drop significantly

This approach isn't scalable (you'd need to recreate the index if you change worker count), but it will definitively show whether a proper index can solve the performance problem.

Run the crawler and check the new EXPLAIN ANALYZE output - it should look much better!

